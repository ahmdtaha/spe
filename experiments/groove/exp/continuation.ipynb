{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import concurrent.futures as cf\n",
    "from collections import defaultdict, deque\n",
    "import copy\n",
    "import io\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from confugue import Configuration\n",
    "import IPython.display as ipd\n",
    "import muspy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import make_interp_spline\n",
    "import scipy.spatial.distance\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "from spe_music.model.music_performer import MusicPerformer\n",
    "from spe_music.model import fast_transformer_decoder\n",
    "from spe_music.train_performer_grv2grv import make_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "fast_transformer_decoder.device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data/train_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = muspy.MusicDataset(DATA_PATH / 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_dir):\n",
    "    cfg = Configuration.from_yaml_file(model_dir / 'config.yaml')\n",
    "    cfg['model']['max_len'] = 2048\n",
    "\n",
    "    representation, start_id, end_id = cfg.configure(make_representation)\n",
    "    model = cfg['model'].configure(MusicPerformer, n_token=len(representation.vocab))\n",
    "\n",
    "    params_path = sorted(model_dir.glob('params/*_params.pt'))[-1]\n",
    "    print(params_path)\n",
    "    state_dict = torch.load(params_path, map_location=torch.device(DEVICE))\n",
    "    if 'pe.pe' in state_dict:\n",
    "        del state_dict['pe.pe']\n",
    "    print(model.load_state_dict(state_dict, strict=False))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return representation, start_id, end_id, model\n",
    "\n",
    "representation, start_id, end_id, model = load_model(model_dir)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, max_len, temperature=0.6, pos_code=None):\n",
    "    tokens = [start_id] + representation.encode(prompt).tolist()[:-1]  # Get rid of EOS\n",
    "    prompt_len = len(tokens)\n",
    "    tokens = tokens + [end_id] * (max_len - len(tokens))\n",
    "    tokens = torch.tensor(tokens, device=DEVICE)[None, :]\n",
    "\n",
    "    active_notes = defaultdict(lambda: defaultdict(deque))\n",
    "    potentially_invalid_ids = set(representation.vocab[('note_off', tr, p)]\n",
    "                                  for tr in range(representation.num_tracks)\n",
    "                                  for p in range(128))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t = 0\n",
    "        for i in range(prompt_len, max_len):\n",
    "            logits = model(tokens[:, :i], attn_kwargs=dict(\n",
    "                omit_feature_map_draw=True, pos_code=pos_code))\n",
    "            logits /= temperature\n",
    "\n",
    "            # Constrain to predict valid tokens\n",
    "            invalid_ids = set(potentially_invalid_ids)\n",
    "            invalid_ids -= set(  # Avoid note-offs for notes that are off\n",
    "                representation.vocab[('note_off', tr, p)]\n",
    "                for tr, d in active_notes.items()\n",
    "                for p in d.keys() if d[p])\n",
    "            invalid_ids.update(  # Avoid backward time shifts\n",
    "                representation.vocab[('time_shift', 0, ticks)]\n",
    "                for ticks in range(1, (t % representation.resolution) + 1)\n",
    "            )\n",
    "            invalid_ids = torch.as_tensor(sorted(invalid_ids), device=DEVICE)\n",
    "            logits.squeeze(0)[-1, invalid_ids] = -1e6\n",
    "\n",
    "            # Sample\n",
    "            dist = torch.distributions.Categorical(logits=logits[:, -1])\n",
    "            tokens[:, i] = dist.sample()\n",
    "            if tokens[:, i] in invalid_ids:\n",
    "                print('Invalid token sampled')\n",
    "            if tokens[:, i] == end_id:\n",
    "                break\n",
    "\n",
    "            # Process event\n",
    "            event, *args = representation.vocab.inv[tokens[:, i].item()]\n",
    "            if event == 'time_shift':\n",
    "                t_new = representation.timing.decode(t, (event, *args))\n",
    "                if t_new <= t:\n",
    "                    print(f'Invalid time shift: {t} + {args} = {t_new}')\n",
    "                else:\n",
    "                    t = t_new\n",
    "            elif event == 'note_on':\n",
    "                track_id, pitch = args\n",
    "                active_notes[track_id][pitch].append(muspy.Note(\n",
    "                    time=t,\n",
    "                    pitch=pitch,\n",
    "                    velocity=-1,\n",
    "                    duration=-1))\n",
    "            elif event == 'note_off':\n",
    "                track_id, pitch = args\n",
    "                try:\n",
    "                    note = active_notes[track_id][pitch].popleft()\n",
    "                except IndexError:\n",
    "                    print('Invalid note-off')\n",
    "                note.duration = t - note.time\n",
    "    \n",
    "    tokens = tokens.cpu().numpy().squeeze(0)\n",
    "    \n",
    "    return representation.decode(tokens), representation.decode(tokens[prompt_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(output, prompt):\n",
    "    m = copy.deepcopy(prompt)\n",
    "    m.tracks.clear()\n",
    "    m += output\n",
    "    for i, track in enumerate(m.tracks):\n",
    "        track.program, track.is_drum = prompt.tracks[i].program, prompt.tracks[i].is_drum\n",
    "        if prompt.tracks[i].notes:\n",
    "            velocity = int(np.mean([n.velocity for n in prompt.tracks[i].notes]))\n",
    "            for note in track.notes:\n",
    "                note.velocity = velocity\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_len = 1024\n",
    "num_outputs = 3\n",
    "\n",
    "for model_dir in ['trio_performer_softmax_l512_v01', 'trio_performer_softmax_sinespe_l512_v01', 'trio_performer_softmax_convspe_l512_v01']:\n",
    "    print(model_dir)\n",
    "    model_dir = Path(model_dir)\n",
    "    representation, start_id, end_id, model = load_model(model_dir)\n",
    "\n",
    "    pos_code = None\n",
    "\n",
    "    torch.random.manual_seed(0)\n",
    "\n",
    "    # Pre-generate positional code for speedup\n",
    "    if model.transformer_decoder._spe and model.transformer_decoder.share_pe:\n",
    "        with torch.no_grad():\n",
    "            pos_code = model.transformer_decoder.spe((1, max_len))\n",
    "\n",
    "    for prompt in tqdm(dataset):\n",
    "        # Prepare prompt\n",
    "        prompt.tracks[2:] = sorted(prompt.tracks[2:], key=lambda tr: -len(tr.notes))\n",
    "        prompt.tracks[:] = prompt.tracks[:3]\n",
    "        style_ref = copy.deepcopy(prompt)\n",
    "        for track in prompt.tracks:\n",
    "            track.notes = [n for n in track.notes if n.end < 2 * 4 * prompt.resolution]\n",
    "\n",
    "        # Save prompt\n",
    "        (model_dir / 'outputs').mkdir(exist_ok=True)\n",
    "        prompt.write_midi(model_dir / 'outputs' / (prompt.metadata.title + '.prompt.mid'))\n",
    "\n",
    "        outputs = []\n",
    "        for j in range(num_outputs):\n",
    "            # Generate\n",
    "            prompt_and_output, output = generate(model, prompt, max_len=max_len, pos_code=pos_code.detach())\n",
    "\n",
    "            # Post-process\n",
    "            prompt_and_output = postprocess(prompt_and_output, prompt)\n",
    "            output = postprocess(output, prompt)\n",
    "\n",
    "            # Save outputs\n",
    "            output.write_midi(model_dir / 'outputs' / (prompt.metadata.title + f'.cont_{j}.mid'))\n",
    "            prompt_and_output.write_midi(model_dir / 'outputs' / (prompt.metadata.title + f'.prompt_cont_{j}.mid'))\n",
    "            outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spe",
   "language": "python",
   "name": "spe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
