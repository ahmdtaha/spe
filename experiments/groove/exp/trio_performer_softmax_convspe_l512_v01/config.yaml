data_loader:
  batch_size: 10
model:
  d_model: 512
  d_embed: 512
  max_len: 512
  add_positional_encoding: False
  decoder:
    n_layer: 24
    n_head: 8
    d_ff: 2048
    feature_map:
      n_dims: 128
    positional_encoder:
      class: !!python/name:spe.ConvSPE
      in_features: 64
      num_realizations: &R 64
      kernel_size: 128
    spe_filter:
      gated: True
    share_pe: True
    share_spe_filter: False
    attention:
      query_dimensions: *R
representation:
  num_tracks: 3
  resolution: 12
timing:
  resolution: 12
  max_shift: 2  # beats

seed: 0
train_data_path: ../data/train_split/train
data_augmentation:
  max_harm_tracks: 1
  harm_tracks_shuffle_prob: 1.
  track_drop_prob: 0.1
training:
  num_epochs: 24
  ckpt_interval: 3  # epochs
  log_interval: 200  # steps

  lr: 4.0e-4
  lr_scheduler:
    class: !!python/name:torch.optim.lr_scheduler.CosineAnnealingLR
    eta_min: 2.0e-5
    T_max: 43184  # 1841 * 24 - 1000
  warmup_steps: 1000

  feature_redraw_interval: 40

val_data_paths:
  val: ../data/train_split/val
  ival: ../data/train_split/ival
val_data_loader:
  batch_size: 8
